{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로지스틱 회귀 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법은 손실을 최소화하는 방법 중 하나!  \n",
    "경사하강법은 선형회귀와 거의 비슷하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^Tx}}\n",
    "$$\n",
    "1. 처음에 $\\theta$값들을 <span style='color:yellowgreen'>모두 0 또는 임의의 값으로 설정</span>한다.  \n",
    "-> 이렇게 되면 현재의 $\\theta$값에 대한 손실을 계산할 수 있음.  \n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{m}\\sum^m_{i=1}[-y^{(i)}\\log(h_\\theta(x^{(i)})) - (1 - y^{(i)})\\log(1 - h_\\theta(x^{(i)}))]\n",
    "$$\n",
    "2. 여기서부터 $\\theta$를 <span style='color:yellowgreen'>업데이트해서 손실을 점점 최소화</span>하는 방식으로 진행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\theta$값을 업데이트하는 방식은 아래와 같다.  \n",
    "(손실함수를 $\\theta$로 편미분한 뒤 학습률 $\\alpha$를 곱한 결과를 기존 \\$theta$값에서 뺀다.)  \n",
    "이렇게 모든 $\\theta$값에 대해서 반복하면 경사하강을 1번 진행한 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/08-1.png\" width=\"700px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 식 $J(\\theta)$에 **로지스틱회귀 손실함수를 대입**하여 계산하면 아래와 같은 결과가 나온다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/08-2.png\" width=\"700px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 선형회귀와 다른 점은 가설함수 $h_\\theta(x^{(i)})$이다.\n",
    "- 선형회귀에서의 가설함수는 1차함수였지만, 로지스틱 회귀에서의 가설함수는 **시그모이드 함수**이기 때문!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
