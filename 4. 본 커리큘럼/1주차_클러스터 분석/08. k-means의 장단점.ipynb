{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means의 장점\n",
    "k-means는 변수들에 대한 배경지식, 역할, 영향도에 대해 모르더라도 데이터 사이의 거리만 구할 수 있다면 쉽게 사용할 수 있습니다.  \n",
    "또, 알고리즘이 비교적 쉬운 수식으로 이루어졌기 때문에 이해와 해석이 용이합니다.\n",
    "\n",
    "이렇게 어떠한 데이터에도 적용하기 쉽고, 모델에 대한 이해와 해석도 간단하게 할 수 있다는 점이 k-means의 장점입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means의 단점\n",
    "먼저, 최적의 클러스터 개수인 k를 정하는게 어렵습니다. 앞에서 배운 Elbow Method로 단서를 얻는 것은 가능하지만,  \n",
    "합리적인 추론을 위한 가이드일 뿐 정답은 아닙니다.\n",
    "\n",
    "또, k-means는 이상치에 영향을 많이 받습니다. 이상치가 포함된 데이터일 경우 클러스터의 중심(Centroid)을 업데이트하는 과정에서  \n",
    "Centroid의 위치가 크게 변동되고, 클러스터가 원하지 않는 방식으로 묶일 수 있습니다.\n",
    "\n",
    "이상치가 많은 데이터라면 전처리 과정에서 전부 제거해 주거나, 아니면 k-means 말고 다른 모델을 사용하는 게 더 좋겠죠?\n",
    "\n",
    "마지막으로, k-means는 초기 Centroid가 어떻게 설정되었는지에 따라 결과가 달라집니다.  \n",
    "또, 적절하지 않은 곳에 비채되면 위치를 너무 많이 옮겨야 해서 연산이 오래 걸릴 수 있고, 경우에 따라서는  \n",
    "특정한 한 곳으로 수렴하지 못하는 경우도 발생할 수 있습니다.\n",
    "\n",
    "이러한 이유 때문에 데이터들 간의 거리를 반영하여 전략적으로 초기 Centroid 위치를 찾아주는 과정이 필요해졌고, 그래서 k-means++ 모델이 등장했습니다.\n",
    "\n",
    "k-means++는 Centroid가 좀 더 좋은 위치에 잘 배치되도록 해 줍니다. 때문에 일반 k-means보다 안정적이죠.\n",
    "\n",
    "사용 방법은 마찬가지로 `KMeans()`를 사용하고, 파라미터로 `init='k-means++'`를 추가해 주면 됩니다.\n",
    "> ```python\n",
    "> from sklearn.cluster import KMeans\n",
    "> model = KMeans(n_clusters=k, init='k-means++')\n",
    "> ```\n",
    "이외에도, 차원이 높은 데이터에 적용할 때 성능이 떨어진다는 단점도 있습니다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
