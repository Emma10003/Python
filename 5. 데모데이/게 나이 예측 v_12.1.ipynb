{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앙상블 모형 개선4.1, K-Fold 사용 (GPT 코드 복붙)  \n",
    "평균 mae = 1.1958"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\emma1\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\emma1\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\emma1\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\emma1\\anaconda3\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: seaborn in c:\\users\\emma1\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\emma1\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\emma1\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\emma1\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\emma1\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\emma1\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\emma1\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\emma1\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\emma1\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\emma1\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\emma1\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\emma1\\anaconda3\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\emma1\\anaconda3\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\emma1\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\emma1\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "🚀 Fold 1 시작...\n",
      "✅ Fold 1 MAE: 1.2014\n",
      "\n",
      "🚀 Fold 2 시작...\n",
      "✅ Fold 2 MAE: 1.2136\n",
      "\n",
      "🚀 Fold 3 시작...\n",
      "✅ Fold 3 MAE: 1.1694\n",
      "\n",
      "🚀 Fold 4 시작...\n",
      "✅ Fold 4 MAE: 1.1951\n",
      "\n",
      "🚀 Fold 5 시작...\n",
      "✅ Fold 5 MAE: 1.1994\n",
      "\n",
      "🔥 K-Fold Cross Validation 평균 MAE: 1.1958\n",
      "✅ sample_submission.csv 파일 생성 완료!\n"
     ]
    }
   ],
   "source": [
    "# 1. 필요한 라이브러리 설치\n",
    "!pip install pandas numpy scikit-learn matplotlib seaborn\n",
    "\n",
    "# 2. 라이브러리 임포트\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "\n",
    "# 3. 데이터 로드\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "submission_df = pd.read_csv(\"data/sample_submission.csv\")\n",
    "\n",
    "# 원본 ID 저장\n",
    "test_ids = test_df[\"id\"].copy()\n",
    "test_df = test_df.drop(columns=[\"id\"])  # 분석을 위해 ID 삭제\n",
    "\n",
    "# 4. 데이터 전처리\n",
    "# 성별(Label Encoding)\n",
    "label_encoder = LabelEncoder()\n",
    "train_df[\"Sex\"] = label_encoder.fit_transform(train_df[\"Sex\"])\n",
    "test_df[\"Sex\"] = label_encoder.transform(test_df[\"Sex\"])\n",
    "\n",
    "# 키(Height) 0 값 평균으로 대체\n",
    "height_mean = train_df.loc[train_df[\"Height\"] > 0, \"Height\"].mean()\n",
    "train_df.loc[train_df[\"Height\"] == 0, \"Height\"] = height_mean\n",
    "test_df.loc[test_df[\"Height\"] == 0, \"Height\"] = height_mean\n",
    "\n",
    "# IQR 기반 이상치 제거\n",
    "def remove_outliers_iqr(df, cols, threshold=1.5):\n",
    "    Q1 = df[cols].quantile(0.25)\n",
    "    Q3 = df[cols].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "    return df[~((df[cols] < lower_bound) | (df[cols] > upper_bound)).any(axis=1)]\n",
    "\n",
    "num_cols = train_df.select_dtypes(include=[\"float64\"]).columns\n",
    "train_df = remove_outliers_iqr(train_df, num_cols, threshold=1.5)\n",
    "\n",
    "# 중복 데이터 제거 및 ID 삭제\n",
    "train_df = train_df.drop_duplicates().drop(columns=[\"id\"])\n",
    "\n",
    "# X, y 분리\n",
    "X = train_df.drop(columns=[\"Age\"])\n",
    "y = train_df[\"Age\"]\n",
    "\n",
    "# 5. K-Fold 설정 (5-Fold 교차 검증)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "mae_scores = []  # MAE 저장 리스트\n",
    "\n",
    "# 반복문으로 K-Fold 실행 (5회)\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n🚀 Fold {fold+1} 시작...\")\n",
    "\n",
    "    # 데이터 분할\n",
    "    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "\n",
    "    # 6. 스케일링\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "    # 7. PCA 적용 (95% 이상의 분산 유지)\n",
    "    pca = PCA(n_components=0.95)\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    X_valid_pca = pca.transform(X_valid_scaled)\n",
    "\n",
    "    # 8. 하이퍼파라미터 튜닝\n",
    "    # 랜덤 포레스트 최적 하이퍼파라미터 찾기\n",
    "    rf_param_grid = {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [5, 10, 20],\n",
    "        \"min_samples_split\": [5, 10],\n",
    "        \"max_features\": [0.5, \"sqrt\", \"log2\"]\n",
    "    }\n",
    "    rf_grid_search = GridSearchCV(RandomForestRegressor(random_state=42), rf_param_grid, \n",
    "                                  scoring=\"neg_mean_absolute_error\", cv=5, n_jobs=-1)\n",
    "    rf_grid_search.fit(X_train_pca, y_train)\n",
    "    rf_best_model = RandomForestRegressor(**rf_grid_search.best_params_, random_state=42)\n",
    "\n",
    "    # 그래디언트 부스팅 최적 하이퍼파라미터 찾기\n",
    "    gb_param_grid = {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "        \"max_depth\": [3, 5, 7],\n",
    "        \"subsample\": [0.7, 0.8, 0.9]\n",
    "    }\n",
    "    gb_grid_search = GridSearchCV(GradientBoostingRegressor(random_state=42), gb_param_grid, \n",
    "                                  scoring=\"neg_mean_absolute_error\", cv=5, n_jobs=-1)\n",
    "    gb_grid_search.fit(X_train_pca, y_train)\n",
    "    gb_best_model = GradientBoostingRegressor(**gb_grid_search.best_params_, random_state=42)\n",
    "\n",
    "    # 9. 앙상블 모델 (Voting Regressor)\n",
    "    lr_model = LinearRegression()\n",
    "    ensemble_model = VotingRegressor(estimators=[\n",
    "        ('lr', lr_model),\n",
    "        ('rf', rf_best_model),\n",
    "        ('gb', gb_best_model)\n",
    "    ], weights=[0.5, 1, 2])\n",
    "\n",
    "    # 10. 모델 학습\n",
    "    ensemble_model.fit(X_train_pca, y_train)\n",
    "\n",
    "    # 11. 검증 데이터 예측 및 평가\n",
    "    y_pred = ensemble_model.predict(X_valid_pca)\n",
    "    fold_mae = mean_absolute_error(y_valid, y_pred)\n",
    "    mae_scores.append(fold_mae)\n",
    "\n",
    "    print(f\"✅ Fold {fold+1} MAE: {fold_mae:.4f}\")\n",
    "\n",
    "# 12. K-Fold 평균 MAE 출력\n",
    "mean_mae = np.mean(mae_scores)\n",
    "print(f\"\\n🔥 K-Fold Cross Validation 평균 MAE: {mean_mae:.4f}\")\n",
    "\n",
    "# 13. 전체 데이터로 최종 학습 후 테스트 예측\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "test_scaled = scaler.transform(test_df)\n",
    "test_pca = pca.transform(test_scaled)\n",
    "\n",
    "# 최적 모델로 최종 학습\n",
    "rf_grid_search.fit(X_pca, y)\n",
    "rf_best_model = RandomForestRegressor(**rf_grid_search.best_params_, random_state=42)\n",
    "\n",
    "gb_grid_search.fit(X_pca, y)\n",
    "gb_best_model = GradientBoostingRegressor(**gb_grid_search.best_params_, random_state=42)\n",
    "\n",
    "ensemble_model = VotingRegressor(estimators=[\n",
    "    ('lr', LinearRegression()),\n",
    "    ('rf', rf_best_model),\n",
    "    ('gb', gb_best_model)\n",
    "], weights=[0.5, 1, 2])\n",
    "\n",
    "ensemble_model.fit(X_pca, y)\n",
    "\n",
    "# 14. 테스트 데이터 예측\n",
    "test_preds = ensemble_model.predict(test_pca)\n",
    "\n",
    "# 15. 제출 파일 생성\n",
    "submission = pd.DataFrame({\"id\": test_ids, \"Age\": np.round(test_preds, 3)})\n",
    "submission.to_csv(\"download/sample_submission.csv\", index=False)\n",
    "print(\"✅ sample_submission.csv 파일 생성 완료!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📌 코드 설명  \n",
    "✅ 패키지 설치: 필요한 라이브러리를 설치하고 import  \n",
    "✅ 데이터 로드 및 전처리: 이상치 제거, 라벨 인코딩, 스케일링  \n",
    "✅ K-Fold 적용: 5개의 Fold로 데이터를 나누어 모델을 평가  \n",
    "✅ GridSearchCV 하이퍼파라미터 튜닝: RandomForest와 GradientBoosting 최적화  \n",
    "✅ Voting Regressor 적용: 세 가지 모델을 조합하여 성능 향상  \n",
    "✅ 최종 예측 및 제출 파일 생성: 최적 모델로 test.csv 예측 후 CSV 저장  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💡 K-Fold를 사용하면...  \n",
    "✔ 모든 데이터를 학습에 활용 → 일반적인 train_test_split()보다 성능이 안정적  \n",
    "✔ 과적합 방지 → 여러 번 평가하여 최적의 모델 선택  \n",
    "✔ 더 정확한 성능 측정 → 여러 번 학습하고 MAE 평균을 계산  \n",
    "\n",
    "이제 K-Fold를 사용한 모델로 나이 예측을 더 정확하게 할 수 있을 거야! 🚀🔥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
