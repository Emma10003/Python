{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앙상블 모형 개선4, K-Fold 사용  \n",
    "mae = 1.0827"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# 1. 데이터 로드\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "submission_df = pd.read_csv(\"data/sample_submission.csv\")\n",
    "\n",
    "# test.csv에서 원본 id 저장\n",
    "test_ids = test_df[\"id\"].copy()\n",
    "test_df = test_df.drop(columns=[\"id\"])  # 이후 분석을 위해 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 데이터 전처리\n",
    "# 'Sex' 라벨 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "train_df[\"Sex\"] = label_encoder.fit_transform(train_df[\"Sex\"])\n",
    "test_df[\"Sex\"] = label_encoder.transform(test_df[\"Sex\"])\n",
    "\n",
    "# Height가 0인 경우 평균값으로 대체\n",
    "height_mean = train_df.loc[train_df[\"Height\"] > 0, \"Height\"].mean()\n",
    "train_df.loc[train_df[\"Height\"] == 0, \"Height\"] = height_mean\n",
    "test_df.loc[test_df[\"Height\"] == 0, \"Height\"] = height_mean\n",
    "\n",
    "# IQR 기반 이상치 제거 함수 정의 및 적용\n",
    "def remove_outliers_iqr(df, cols, threshold=1.5):  # threshold=3.0에서 변경\n",
    "    Q1 = df[cols].quantile(0.25)\n",
    "    Q3 = df[cols].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "    return df[~((df[cols] < lower_bound) | (df[cols] > upper_bound)).any(axis=1)]\n",
    "\n",
    "num_cols = train_df.select_dtypes(include=[\"float64\"]).columns\n",
    "train_df = remove_outliers_iqr(train_df, num_cols, threshold=1.5)\n",
    "\n",
    "# 중복 데이터 제거 및 'id' 컬럼 삭제\n",
    "train_df = train_df.drop_duplicates().drop(columns=[\"id\"])\n",
    "\n",
    "# X, y 분리\n",
    "X = train_df.drop(columns=[\"Age\"])\n",
    "y = train_df[\"Age\"]\n",
    "\n",
    "# # Train/Validation Split (8:2 비율)\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. 스케일링\n",
    "scaler = StandardScaler()  # MinMaxScaler()에서 변경\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "test_scaled = scaler.transform(test_df)  # test data도 스케일링 적용\n",
    "\n",
    "# 4. PCA 적용 (95% 이상의 분산을 설명하는 주성분 선택)\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_valid_pca = pca.transform(X_valid_scaled)\n",
    "test_pca = pca.transform(test_scaled)  # test data에도 pca 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RandomForest Params: {'max_depth': 10, 'max_features': 0.5, 'min_samples_split': 10, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# 5. 개별 모델 최적화 (GridSearchCV 적용) - 최적의 하이퍼파라미터\n",
    "# RandomForestRegressor와 GradientBoostingRegressor에만 적용\n",
    "\n",
    "# 5-1. RandomForestRegressor 튜닝\n",
    "rf_param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [5, 10, 20], # 기존보다 낮춰서 과적합 방지\n",
    "    \"min_samples_split\": [5, 10],  # 더 큰 값으로 설정해 분할 제한\n",
    "    \"max_features\" : [0.5, \"sqrt\", \"log2\"]  # 특성 수 제한\n",
    "}\n",
    "rf_grid_search = GridSearchCV(RandomForestRegressor(random_state=42), rf_param_grid, \n",
    "                              scoring=\"neg_mean_absolute_error\", cv=5, n_jobs=-1)\n",
    "rf_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Best RandomForest Params: {rf_grid_search.best_params_}\")\n",
    "\n",
    "# 개별 모델 정의, 최적모델 할당\n",
    "rf_model = RandomForestRegressor(**rf_grid_search.best_params_, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GradientBoosting Params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# 5-2. GradientBoostingRegressor 튜닝\n",
    "gb_param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"subsample\" : [0.7, 0.8, 0.9]  # 과적합 방지용\n",
    "}\n",
    "gb_grid_search = GridSearchCV(GradientBoostingRegressor(random_state=42), gb_param_grid, \n",
    "                              scoring=\"neg_mean_absolute_error\", cv=5, n_jobs=-1)\n",
    "gb_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Best GradientBoosting Params: {gb_grid_search.best_params_}\")\n",
    "\n",
    "# 개별 모델 정의, 최적모델 할당\n",
    "gb_model = GradientBoostingRegressor(**gb_grid_search.best_params_, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE with Voting Regressor: 1.0827\n"
     ]
    }
   ],
   "source": [
    "# 6. 앙상블 모델 (Voting Regressor) 설정\n",
    "# 개별 모델 정의\n",
    "# RandomForest와 GradientBoosting은 위에서 튜닝하면서 정의함.\n",
    "lr_model = LinearRegression()  # 선형회귀\n",
    "\n",
    "# Voting Regressor 설정 (모델들을 평균으로 결합)\n",
    "ensemble_model = VotingRegressor(estimators=[\n",
    "    ('lr', lr_model),\n",
    "    ('rf', rf_model),\n",
    "    ('gb', gb_model)\n",
    "], weights=[0.5, 1, 2])  # 가중치 조정, GradientBoostingRegressor에 더 높은 가중치 부여\n",
    "\n",
    "# 7. 모델 학습\n",
    "ensemble_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 8. 검증 데이터 예측\n",
    "y_pred = ensemble_model.predict(X_valid_scaled)\n",
    "mae = mean_absolute_error(y_valid, y_pred)\n",
    "print(f\"Validation MAE with Voting Regressor: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv 파일 생성 완료!\n"
     ]
    }
   ],
   "source": [
    "# 9. 테스트 데이터 예측\n",
    "test_preds = ensemble_model.predict(test_scaled)\n",
    "\n",
    "# 10. 제출 파일 생성 (원래 id 유지)\n",
    "submission = pd.DataFrame({\"id\": test_ids, \"Age\": np.round(test_preds, 3)})\n",
    "submission.to_csv(\"download/sample_submission.csv\", index=False)\n",
    "print(\"sample_submission.csv 파일 생성 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "코드 뜯어보기\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `VotingRegressor` : 여러 개의 개별 **회귀모델**을 조합하여 최종 예측값을 생성하는 앙상블 방법 중 하나, 각 모델의 예측값을 평균냄.\n",
    "\n",
    "```python\n",
    "# 개별 모델들 정의\n",
    "```\n",
    "- `LinearRegression()` : 선형회귀, 데이터의 선형적 관계 학습\n",
    "    - 데이터가 정규화되지 않으면 성능이 저하될 수 있음. -> 스케일링 필요\n",
    "- `RandomForestRegressor(n_estimators=100, random_state=42)` : 랜덤포레스트회귀, 여러 결정트리를 조합해 예측력 높임\n",
    "    - `max_depth` : 트리의 깊이 제한 - 과적합 방지\n",
    "    - `min_sample_split` : 최소한 몇 개의 샘플이 있어야 노드를 분할할지 설정\n",
    "    - `min_sample_leaf` : 리프 노트에 필요한 최소 샘플 개수 지정.\n",
    "\n",
    "- `GradientBoostingRegression(n_estimators=100, random_state=42)` : 여러 개의 약한 모델(결정트리)를 순차적으로 학습해 예측력 향상\n",
    "    - `learning_rate` : 학습 속도 줄이면 모델이 점진적으로 개선 -> 과적합 방지 가능\n",
    "    - `n_estimators` : 너무 크면 과적합될 수 있음 (적절한 값 찾기)\n",
    "    - `subsample` : 1보다 작은 값 설정 -> 일부 데이터만 샘플링하여 훈련하면 과적합 방지"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
